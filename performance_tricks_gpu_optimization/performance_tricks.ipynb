{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet Performance tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    " \n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import numpy as np\n",
    "import random \n",
    "import mxboard as mxb\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs && mkdir logs\n",
    "sw = mxb.SummaryWriter(logdir='logs', flush_secs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We get a relatively simple and common model architecture from the model the zoo, ResNet50 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "net = gluon.model_zoo.vision.resnet50_v2(pretrained=False, ctx=ctx)\n",
    "net.initialize(mx.init.Xavier(magnitude=2.3), ctx=ctx)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x, y):\n",
    "    x = resize(x.asnumpy(), (224, 224), anti_aliasing=False, mode='constant')\n",
    "    x = x.transpose((2, 0, 1)).astype('float32')\n",
    "    return x, y\n",
    "dataset_train = gluon.data.vision.CIFAR10(train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = gluon.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False, last_batch=\"discard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':0.001, 'momentum':0.9, 'wd':0.00001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Naive Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 1\n",
    "print_n = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    tick = time.time()\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        if i == 0:\n",
    "            tick_0 = time.time()\n",
    "            \n",
    "        # Forward pass and loss computation\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "        \n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # Update metric\n",
    "        accuracy.update(label, output)\n",
    "        \n",
    "        # Print batch metrics\n",
    "        if i % print_n == 0 and i > 0:\n",
    "            sw.add_scalar(tag='Accuracy', value={'naive':accuracy.get()[1]}, global_step=i-print_n)\n",
    "            sw.add_scalar(tag='Speed', value={'naive':data.shape[0]*(print_n)/(time.time()-tick)}, global_step=i-print_n)\n",
    "            print('Batch [{}], Accuracy {:.4f}, Samples/sec: {:.4f}'.format(\n",
    "                i, accuracy.get()[1], data.shape[0]*(print_n)/(time.time()-tick))\n",
    "            )\n",
    "            tick = time.time()\n",
    "        \n",
    "        if i == 200:\n",
    "            break\n",
    "            \n",
    "    print('Epoch [{}], Accuracy {:.4f}'.format(e, accuracy.get()[1]))\n",
    "    print('~Samples/Sec {:.4f}'.format(data.shape[0]*(i+1)/(time.time()-tick_0)))\n",
    "    accuracy.reset()\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we are I/O constrained, very low GPU util, drops in GPU util and high CPU utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Using multiprocessing workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = gluon.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False, last_batch=\"discard\", \n",
    "                                         num_workers=multiprocessing.cpu_count()-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(magnitude=2.3), ctx=ctx, force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 1\n",
    "print_n = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    tick = time.time()\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        if i == 0:\n",
    "            tick_0 = time.time()\n",
    "        \n",
    "        # Forward pass and loss computation\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "        \n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # Update metric\n",
    "        accuracy.update(label, output)\n",
    "        \n",
    "        # Print batch metrics\n",
    "        if i % print_n == 0 and i > 0:\n",
    "            sw.add_scalar(tag='Accuracy', value={'multi':accuracy.get()[1]}, global_step=i-print_n)\n",
    "            sw.add_scalar(tag='Speed', value={'multi':data.shape[0]*(print_n)/(time.time()-tick)}, global_step=i-print_n)\n",
    "            print('Batch [{}], Accuracy {:.4f}, Samples/sec: {:.4f}'.format(\n",
    "                i, accuracy.get()[1], data.shape[0]*(print_n)/(time.time()-tick))\n",
    "            )\n",
    "            tick = time.time()\n",
    "        \n",
    "        if i == 200:\n",
    "            break\n",
    "            \n",
    "    print('Epoch [{}], Accuracy {:.4f}'.format(e, accuracy.get()[1]))\n",
    "    print('~Samples/Sec {:.4f}'.format(data.shape[0]*(i+1)/(time.time()-tick_0)))\n",
    "    accuracy.reset()\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice now that CPU utilization seems to be less than 100% so data fetching and resizing is no more the bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(magnitude=2.3), ctx=ctx, force_reinit=True)\n",
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "out = net(mx.nd.ones((1, 3, 224, 224), ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 1\n",
    "print_n = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    tick = time.time()\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        if i == 0:\n",
    "            tick_0 = time.time()\n",
    "            \n",
    "        # Forward pass and loss computation\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "        \n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # Update metric\n",
    "        accuracy.update(label, output)\n",
    "        \n",
    "        # Print batch metrics\n",
    "        if i % print_n == 0 and i > 0:\n",
    "            sw.add_scalar(tag='Accuracy', value={'hybrid':accuracy.get()[1]}, global_step=i-print_n)\n",
    "            sw.add_scalar(tag='Speed', value={'hybrid':data.shape[0]*(print_n)/(time.time()-tick)}, global_step=i-print_n)\n",
    "            print('Batch [{}], Accuracy {:.4f}, Samples/sec: {:.4f}'.format(\n",
    "                i, accuracy.get()[1], data.shape[0]*(print_n)/(time.time()-tick))\n",
    "            )\n",
    "            tick = time.time()\n",
    "        \n",
    "        if i == 200:\n",
    "            break\n",
    "            \n",
    "    print('Epoch [{}], Accuracy {:.4f}'.format(e, accuracy.get()[1]))\n",
    "    print('~Samples/Sec {:.4f}'.format(data.shape[0]*(i+1)/(time.time()-tick_0)))\n",
    "    accuracy.reset()\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Tweaking hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 96\n",
    "dataloader_train = gluon.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False, last_batch=\"discard\", num_workers=multiprocessing.cpu_count()-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(magnitude=2.3), ctx=ctx, force_reinit=True)\n",
    "trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':0.001*2, 'momentum':0.9, 'wd':0.00001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 1\n",
    "print_n = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    tick = time.time()\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        if i == 0:\n",
    "            tick_0 = time.time()\n",
    "            \n",
    "        # Forward pass and loss computation\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "        \n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # Update metric\n",
    "        accuracy.update(label, output)\n",
    "        \n",
    "        # Print batch metrics\n",
    "        if i % print_n == 0 and i > 0:\n",
    "            sw.add_scalar(tag='Accuracy', value={'hybrid':accuracy.get()[1]}, global_step=i-print_n)\n",
    "            sw.add_scalar(tag='Speed', value={'hybrid':data.shape[0]*(print_n)/(time.time()-tick)}, global_step=i-print_n)\n",
    "            print('Batch [{}], Accuracy {:.4f}, Samples/sec: {:.4f}'.format(\n",
    "                i, accuracy.get()[1], data.shape[0]*(print_n)/(time.time()-tick))\n",
    "            )\n",
    "            tick = time.time()\n",
    "        \n",
    "        if i == 100:\n",
    "            break\n",
    "            \n",
    "    print('Epoch [{}], Accuracy {:.4f}'.format(e, accuracy.get()[1]))\n",
    "    print('~Samples/Sec {:.4f}'.format(data.shape[0]*(i+1)/(time.time()-tick_0)))\n",
    "    accuracy.reset()\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Synchronization calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(1)\n",
    "net = gluon.model_zoo.vision.resnet50_v2(pretrained=False, ctx=ctx)\n",
    "net.initialize(mx.init.Xavier(magnitude=2.3), ctx=ctx)\n",
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "out = net(mx.nd.ones((1, 3, 224, 224), ctx))\n",
    "trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':0.001*2, 'momentum':0.9, 'wd':0.00001})\n",
    "accuracy = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 1\n",
    "print_n_sync = 2\n",
    "print_n = 6\n",
    "tick_0 = time.time()\n",
    "\n",
    "for e in range(epoch):\n",
    "    tick = time.time()\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        if i == 0:\n",
    "            tick_0 = time.time()\n",
    "\n",
    "        # Update metric\n",
    "        if i % print_n_sync == 0 and i > 0:\n",
    "            accuracy.update(old_label, output)\n",
    "            \n",
    "        old_label = label\n",
    "        \n",
    "        # Print batch metrics            \n",
    "        if i % print_n == 0 and i > 0:\n",
    "            sw.add_scalar(tag='Accuracy', value={'hybrid_sync':accuracy.get()[1]}, global_step=i-print_n)\n",
    "            sw.add_scalar(tag='Speed', value={'hybrid_sync':data.shape[0]*(print_n)/(time.time()-tick)}, global_step=i-print_n)\n",
    "\n",
    "            print('Batch [{}], Accuracy {:.4f}, Samples/sec: {:.4f}'.format(\n",
    "                i, accuracy.get()[1], data.shape[0]*(print_n)/(time.time()-tick))\n",
    "            )\n",
    "            tick = time.time()\n",
    "            \n",
    "        # Forward pass and loss computation\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "            \n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        \n",
    "        if i == 100:\n",
    "            break\n",
    "    print('Epoch [{}], Accuracy {:.4f}'.format(e, accuracy.get()[1]))\n",
    "    print('~Samples/Sec {:.4f}'.format(data.shape[0]*(i+1)/(time.time()-tick_0)))\n",
    "    accuracy.reset()\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(magnitude=2.3), ctx=ctx, force_reinit=True)\n",
    "net.cast('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':0.001*2, 'momentum':0.9, 'wd':0.00001, 'multi_precision':True})\n",
    "accuracy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch = 1\n",
    "print_n_sync = 2\n",
    "print_n = 5\n",
    "tick_0 = time.time()\n",
    "\n",
    "for e in range(epoch):\n",
    "    tick = time.time()\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.as_in_context(ctx).astype('float16')\n",
    "        label = label.as_in_context(ctx).astype('float16')\n",
    "\n",
    "        # Update metric\n",
    "        if i % print_n_sync == 0 and i > 0:\n",
    "            accuracy.update(old_label, output)\n",
    "        \n",
    "        old_label = label\n",
    "        \n",
    "        if i == 0:\n",
    "            tick_0 = time.time()\n",
    "            \n",
    "        # Print batch metrics            \n",
    "        if i % print_n == 0 and i > 0:\n",
    "            sw.add_scalar(tag='Accuracy', value={'float_16':accuracy.get()[1]}, global_step=i-print_n)\n",
    "            sw.add_scalar(tag='Speed', value={'float_16':data.shape[0]*(print_n)/(time.time()-tick)}, global_step=i-print_n)\n",
    "\n",
    "            print('Batch [{}], Accuracy {:.4f}, Samples/sec: {:.4f}'.format(\n",
    "                i, accuracy.get()[1], data.shape[0]*(print_n)/(time.time()-tick))\n",
    "            )\n",
    "            tick = time.time()\n",
    "            \n",
    "        # Forward pass and loss computation\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "            \n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update network weights\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # Update metric\n",
    "        if i % print_n_sync == 0:\n",
    "            accuracy.update(label, output)\n",
    "\n",
    "        \n",
    "        if i == 100:\n",
    "            break\n",
    "    print('Epoch [{}], Accuracy {:.4f}'.format(e, accuracy.get()[1]))\n",
    "    print('~Samples/Sec {:.4f}'.format(data.shape[0]*(i+1)/(time.time()-tick_0)))\n",
    "    accuracy.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "sw.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Good datapoint: Testing theoritical maximum speed without I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 10\n",
    "tick_0 = time.time()\n",
    "data = mx.nd.ones((BATCH_SIZE,3,224,224), ctx=ctx, dtype='float16')\n",
    "for i in range(batches):\n",
    "    with autograd.record():\n",
    "        out = net(data)\n",
    "    out.backward()\n",
    "    trainer.step(data.shape[0])\n",
    "out.asnumpy()\n",
    "mx.nd.waitall()\n",
    "print('Max Sample Speed {:.4f}'.format(batches*BATCH_SIZE/(time.time()-tick_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Variables\n",
    "\n",
    "https://mxnet.incubator.apache.org/faq/env_var.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
